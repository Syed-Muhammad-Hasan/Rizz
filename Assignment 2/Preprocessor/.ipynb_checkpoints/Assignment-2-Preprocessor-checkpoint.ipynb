{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a32840b-f217-49cc-9567-5a31e02fe14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import csv \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36048dd-2584-4381-9120-d791c2f85f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albuquerque', 'Anaheim', 'Arlington', 'Atlanta', 'Aurora', 'Austin', 'Baltimore', 'Boston', 'Buffalo', 'CapeCoral', 'ColoradoSprings', 'Columbus', 'Dallas', 'Denver', 'DesMoines', 'Detroit', 'Durham', 'Fresno', 'GardenGrove', 'GrandRapids', 'Greensboro', 'Honolulu', 'Houston', 'HuntingtonBeach', 'Indianapolis', 'Irvine', 'Jerseycity', 'Knoxville', 'LasVegas', 'LosAngeles', 'Louisville', 'Madison', 'Miami', 'Milwaukee', 'Minneapolis', 'Nashville', 'NewOrleans', 'NewYork', 'Oakland', 'OklahomaCity', 'Ontario', 'Orlando', 'OverlandPark', 'Phoenix', 'Pittsburgh', 'Plano', 'Portland', 'Providence', 'RanchoCucamonga', 'Richmond', 'Rochester', 'Sacramento', 'SanDiego', 'SanFrancisco', 'SanJose', 'SantaRosa', 'Seattle', 'SiouxFalls', 'StLouis', 'Stockton', 'Tampa', 'WashingtonDC', 'Worcester']\n"
     ]
    }
   ],
   "source": [
    "#Get filenames from the directory for setting up the dropdown.\n",
    "\n",
    "FullFilenames = (glob.glob(\"../../Dataset/*.csv\"))\n",
    "OnlyFilenames = []\n",
    "for i in FullFilenames:\n",
    "    SplitOnUnderscores = i.split('_')\n",
    "    SplitOnSlash = SplitOnUnderscores[0].split('\\\\')\n",
    "    filename = SplitOnSlash[1]\n",
    "    OnlyFilenames.append(filename)\n",
    "print(OnlyFilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59792422-c189-4c51-b8e5-45b15a445fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scientific_name', 'common_name', 'city', 'state']\n"
     ]
    }
   ],
   "source": [
    "# insert the names of columns you wish to extract Intresting columns are \n",
    "# the ones for which we will visualise the data. If all rows of any intresting columns have null values we will drop the data file.\n",
    "\n",
    "IntrestingColumns = ['scientific_name','common_name','city','state'] \n",
    "print(IntrestingColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c683541a-987d-4316-a270-8ab15a578b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Api Call to get the family name by scientific name\n",
    "def getPlantFamily(plantName):\n",
    "    url = \"https://api.gbif.org/v1/species?name=\"+plantName\n",
    "    myResponse = requests.get(url)\n",
    "    Data = json.loads(myResponse.content)\n",
    "    results = Data['results']\n",
    "    for i in results:\n",
    "        if 'family' in i:\n",
    "            return i['family']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6520776-e031-4163-aba0-61e645e741f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'New Mexico': ['Albuquerque'], 'California': ['Anaheim', 'Fresno', 'GardenGrove', 'HuntingtonBeach', 'Irvine', 'LosAngeles', 'Ontario', 'RanchoCucamonga', 'Sacramento', 'SanDiego', 'SanFrancisco', 'SantaRosa', 'Stockton'], 'Colorado': ['Aurora', 'ColoradoSprings', 'Denver'], 'Texas': ['Austin', 'Dallas', 'Houston', 'Plano'], 'Maryland': ['Baltimore'], 'Massachusetts': ['Boston'], 'New York': ['Buffalo', 'NewYork', 'Rochester'], 'Florida': ['CapeCoral', 'Miami', 'Orlando', 'Tampa'], 'Ohio': ['Columbus'], 'Iowa': ['DeSoto'], 'Michigan': ['Detroit', 'GrandRapids'], 'North Carolina': ['Durham', 'Greensboro'], 'Hawaii': ['Kailua'], 'Tennessee': ['Knoxville', 'Nashville'], 'Nevada': ['LasVegas'], 'Kentucky': ['Louisville'], 'Wisconsin': ['Madison', 'Cedarburg'], 'Louisiana': ['NewOrleans'], 'Oklahoma': ['OklahomaCity'], 'Kansas': ['OverlandPark'], 'Arizona': ['Phoenix'], 'Pennsylvania': ['Pittsburgh'], 'Oregon': ['Portland'], 'Virginia': ['Richmond'], 'Washington': ['Seattle'], 'South Dakota': ['SiouxFalls'], 'Missouri': ['St.Louis'], 'District of Columbia': ['WashingtonDC']}\n"
     ]
    }
   ],
   "source": [
    "# Generate the csv which will be used for sankey diagram development\n",
    "def processFile():\n",
    "    StateData = pd.DataFrame(columns=['source','target','value'])\n",
    "    State_city_dictionary = dict()\n",
    "    Cities = (glob.glob(\"../../Dataset/*.csv\"))\n",
    "    for city in Cities:\n",
    "        data = pd.read_csv(city, usecols=IntrestingColumns) #use nrows paramter to limit the data if it consumes too much time to process\n",
    "        data.dropna(how='any', inplace=True)\n",
    "        \n",
    "        if(not(data.empty)):\n",
    "            # Extract city and state information\n",
    "            city_state_info = data.iloc[0][['city', 'state']].to_dict()\n",
    "            if city_state_info['state'] not in State_city_dictionary:\n",
    "                    State_city_dictionary[city_state_info['state']] = [city_state_info['city'].replace(\" \", \"\")]\n",
    "            else:\n",
    "                State_city_dictionary[city_state_info['state']].append(city_state_info['city'].replace(\" \", \"\"))\n",
    "                \n",
    "    for state, cities in State_city_dictionary.items():\n",
    "        for city in cities:\n",
    "            Filename = (glob.glob(\"../../Dataset/\" + city + \"*.csv\"))\n",
    "            if(city =='DeSoto'):\n",
    "                Filename = (glob.glob(\"../../Dataset/DesMoines*.csv\"))\n",
    "            \n",
    "            data = pd.read_csv(Filename[0], usecols=IntrestingColumns) #use nrows paramter to limit the data if it consumes too much time to process\n",
    "            data.dropna(how='any', inplace=True)\n",
    "            \n",
    "            if(not(data.empty)):\n",
    "                # Group data by scientific_name and get the count\n",
    "                tree_counts = data['scientific_name'].value_counts().to_dict()\n",
    "        \n",
    "                # Sort the dictionary by values in descending order\n",
    "                sorted_tree_counts = dict(sorted(tree_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "        \n",
    "                # Get the top 5 items\n",
    "                top_5_trees = dict(list(sorted_tree_counts.items())[:5])\n",
    "                \n",
    "                \n",
    "                family_plant_count = pd.DataFrame(columns=['source','target','value'])\n",
    "                for key,value in top_5_trees.items():\n",
    "                    familyName = getPlantFamily(key)\n",
    "                    family_plant_count = pd.concat([pd.DataFrame([[familyName,key,value]], columns=family_plant_count.columns), family_plant_count], ignore_index=True)\n",
    "        \n",
    "                groupBySource = family_plant_count.groupby('source').agg( {\"value\":\"sum\"} ).reset_index()\n",
    "                \n",
    "                city_family_count = pd.DataFrame({\n",
    "                    'source': city,\n",
    "                    'target':  groupBySource['source'], \n",
    "                    'value': groupBySource['value']\n",
    "                })\n",
    "                StateData = pd.concat([family_plant_count, StateData], ignore_index=True)\n",
    "                StateData = pd.concat([city_family_count, StateData], ignore_index=True)\n",
    "                \n",
    "        \n",
    "        StateCityDataframe = pd.DataFrame(columns=['source','target','value'])\n",
    "        for key, val in State_city_dictionary.items():\n",
    "            if len(val) >1 :\n",
    "                for i in val:\n",
    "                    StateCityDataframe = pd.concat([pd.DataFrame([[key,i,1]], columns=StateCityDataframe.columns), StateCityDataframe], ignore_index=True)\n",
    "            StateCityDataframe = pd.concat([pd.DataFrame([[key,val[0],1]], columns=StateCityDataframe.columns), StateCityDataframe], ignore_index=True)\n",
    "    \n",
    "        StateData = pd.concat([StateCityDataframe, StateData], ignore_index=True)\n",
    "        StateData.dropna(how='any', inplace=True)       \n",
    "        StateData.to_csv(state+'.csv', index = False)\n",
    "processFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f13ac-8983-4ad9-b33c-b11ba9e1af78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
